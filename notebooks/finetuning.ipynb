{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install dataset evaluate scikit-learn\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv(\"train_cleaned_v2.csv\")\n",
    "test = pd.read_csv(\"test_cleaned_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Cyber Bullying  Stalking  Sexting',\n",
       " 1: 'Fraud CallVishing',\n",
       " 2: 'Online Gambling  Betting',\n",
       " 3: 'Online Job Fraud',\n",
       " 4: 'UPI Related Frauds',\n",
       " 5: 'Internet Banking Related Fraud',\n",
       " 6: 'Other',\n",
       " 7: 'Profile Hacking Identity Theft',\n",
       " 8: 'DebitCredit Card FraudSim Swap Fraud',\n",
       " 9: 'EWallet Related Fraud',\n",
       " 10: 'Data Breach/Theft',\n",
       " 11: 'Denial of Service (DoS)/Distributed Denial of Service (DDOS) attacks',\n",
       " 12: 'FakeImpersonating Profile',\n",
       " 13: 'Cryptocurrency Fraud',\n",
       " 14: 'Malware Attack',\n",
       " 15: 'Business Email CompromiseEmail Takeover',\n",
       " 16: 'Email Hacking',\n",
       " 17: 'Cheating by Impersonation',\n",
       " 18: 'Hacking/Defacement',\n",
       " 19: 'Unauthorised AccessData Breach',\n",
       " 20: 'SQL Injection',\n",
       " 21: 'Provocative Speech for unlawful acts',\n",
       " 22: 'Ransomware Attack',\n",
       " 23: 'Cyber Terrorism',\n",
       " 24: 'Tampering with computer source documents',\n",
       " 25: 'DematDepository Fraud',\n",
       " 26: 'Online Trafficking',\n",
       " 27: 'Online Matrimonial Fraud',\n",
       " 28: 'Website DefacementHacking',\n",
       " 29: 'Damage to computer computer systems etc',\n",
       " 30: 'Impersonating Email',\n",
       " 31: 'EMail Phishing',\n",
       " 32: 'Ransomware',\n",
       " 33: 'Intimidating Email',\n",
       " 34: 'Against Interest of sovereignty or integrity of India'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train['sub_category'].unique().tolist()\n",
    "labels = [s.strip() for s in labels ]\n",
    "NUM_LABELS= len(labels)\n",
    "\n",
    "id2label={id:label for id,label in enumerate(labels)}\n",
    "label2id={label:id for id,label in enumerate(labels)}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, \n",
    "                                                           num_labels=NUM_LABELS, \n",
    "                                                           id2label=id2label, \n",
    "                                                           label2id=label2id,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze base model parameters\n",
    "for name, param in model.base_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze base model pooling layers\n",
    "for name, param in model.base_model.named_parameters():\n",
    "    if \"pooler\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4610d27e437f4225822a8a43966152b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85892 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07f42b4588240c386c53ada15de4a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28619 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['crimeaditionalinfo'] = train['crimeaditionalinfo'].astype(str)\n",
    "test['crimeaditionalinfo'] = test['crimeaditionalinfo'].astype(str)\n",
    "\n",
    "labels = train['sub_category'].unique().tolist()\n",
    "label_to_id = {label: idx for idx, label in enumerate(labels)}\n",
    "train['label'] = train['sub_category'].map(label_to_id)\n",
    "test['label'] = test['sub_category'].map(label_to_id)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train[['crimeaditionalinfo', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test[['crimeaditionalinfo', 'label']])\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"crimeaditionalinfo\"], truncation=True)\n",
    "\n",
    "tokenized_data = dataset_dict.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    print(pred)\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "batch_size = 32\n",
    "num_epochs = 32\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert-cyberguard-classifier-V2\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
