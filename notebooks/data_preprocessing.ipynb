{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, re, warnings, pickle, itertools, emoji, psutil, random, unicodedata\n",
    "\n",
    "# custom imports\n",
    "from gensim.utils import deaccent\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "########################### Helpers####################################\n",
    "## Multiprocessing Run.\n",
    "# :df - DataFrame to split                      # type: pandas DataFrame\n",
    "# :func - Function to apply on each split       # type: python function\n",
    "# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n",
    "def df_parallelize_run(df, func):\n",
    "    num_partitions, num_cores = 16, psutil.cpu_count()  # number of partitions and cores\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "## Build of vocabulary from file - reading data line by line\n",
    "## Line splited by 'space' and we store just first argument - Word\n",
    "# :path - txt/vec/csv absolute file path        # type: str\n",
    "def get_vocabulary(path):\n",
    "    with open(path) as f:\n",
    "        return [line.strip().split()[0] for line in f][0:]\n",
    "\n",
    "## Check how many words are in Vocabulary\n",
    "# :c_list - 1d array with 'comment_text'        # type: pandas Series\n",
    "# :vocabulary - words in vocabulary to check    # type: list of str\n",
    "# :response - type of response                  # type: str\n",
    "def check_vocab(c_list, vocabulary, response='default'):\n",
    "    try:\n",
    "        words = set([w for line in c_list for w in line.split()])\n",
    "        u_list = words.difference(set(vocabulary))\n",
    "        k_list = words.difference(u_list)\n",
    "    \n",
    "        if response=='default':\n",
    "            print('Unknown words:', len(u_list), '| Known words:', len(k_list))\n",
    "        elif response=='unknown_list':\n",
    "            return list(u_list)\n",
    "        elif response=='known_list':\n",
    "            return list(k_list)\n",
    "    except:\n",
    "        return []\n",
    "        \n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if 'torch' in sys.modules:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    " \n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "    \n",
    "## Export pickle\n",
    "def make_export(tr, tt, file_name):\n",
    "    train_export = train[['id']]\n",
    "    test_export = test[['id']]\n",
    "\n",
    "    try:\n",
    "        cur_shape = tr.shape[1]>1\n",
    "        train_export = pd.concat([train_export, tr], axis=1)\n",
    "        test_export = pd.concat([test_export, tt], axis=1)        \n",
    "    except:\n",
    "        train_export['p_comment'] = tr\n",
    "        test_export['p_comment'] = tt\n",
    "    \n",
    "    train_export.to_pickle(file_name + '_x_train.pkl')\n",
    "    test_export.to_pickle(file_name + '_x_test.pkl')\n",
    "\n",
    "## Domain Search\n",
    "re_3986_enhanced = re.compile(r\"\"\"\n",
    "        # Parse and capture RFC-3986 Generic URI components.\n",
    "        ^                                    # anchor to beginning of string\n",
    "        (?:  (?P<scheme>    [^:/?#\\s]+):// )?  # capture optional scheme\n",
    "        (?:(?P<authority>  [^/?#\\s]*)  )?  # capture optional authority\n",
    "             (?P<path>        [^?#\\s]*)      # capture required path\n",
    "        (?:\\?(?P<query>        [^#\\s]*)  )?  # capture optional query\n",
    "        (?:\\#(?P<fragment>      [^\\s]*)  )?  # capture optional fragment\n",
    "        $                                    # anchor to end of string\n",
    "        \"\"\", re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "re_domain =  re.compile(r\"\"\"\n",
    "        # Pick out top two levels of DNS domain from authority.\n",
    "        (?P<domain>[^.]+\\.[A-Za-z]{2,6})  # $domain: top two domain levels.\n",
    "        (?::[0-9]*)?                      # Optional port number.\n",
    "        $                                 # Anchor to end of string.\n",
    "        \"\"\", \n",
    "        re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "def domain_search(text):\n",
    "    try:\n",
    "        return re_domain.search(re_3986_enhanced.match(text).group('authority')).group('domain')\n",
    "    except:\n",
    "        return 'url'\n",
    "\n",
    "## Load helper helper))\n",
    "def load_helper_file(filename):\n",
    "    with open(HELPER_PATH+filename+'.pickle', 'rb') as f:\n",
    "        temp_obj = pickle.load(f)\n",
    "    return temp_obj\n",
    "        \n",
    "## Preprocess helpers\n",
    "def place_hold(w):\n",
    "    return WPLACEHOLDER + '['+re.sub(' ', '___', w)+']'\n",
    "\n",
    "def check_replace(w):\n",
    "    return not bool(re.search(WPLACEHOLDER, w))\n",
    "\n",
    "def make_cleaning(s, c_dict):\n",
    "    if check_replace(s):\n",
    "        s = s.translate(c_dict)\n",
    "    return s\n",
    "  \n",
    "def make_dict_cleaning(s, w_dict):\n",
    "    if check_replace(s):\n",
    "        s = w_dict.get(s, s)\n",
    "    return s\n",
    "\n",
    "def export_dict(temp_dict, serial_num):\n",
    "    pd.DataFrame.from_dict(temp_dict, orient='index').to_csv('dict_'+str(serial_num)+'.csv')\n",
    "\n",
    "def print_dict(temp_dict, n_items=10):\n",
    "    run = 0\n",
    "    for k,v in temp_dict.items():\n",
    "        print(k,'---',v)\n",
    "        run +=1\n",
    "        if run==n_items:\n",
    "            break    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1. Load Data\n",
      "1.2. Basic helpers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "HELPER_PATH             = 'data/helper/'\n",
    "\n",
    "LOCAL_TEST = True       ## Local test - for test performance on part of the train set only\n",
    "SEED = 42               ## Seed for enviroment\n",
    "seed_everything(SEED)   ## Seed everything\n",
    "\n",
    "WPLACEHOLDER = 'word_placeholder'\n",
    "\n",
    "########################### DATA LOAD #################################################################################\n",
    "print('1.1. Load Data')\n",
    "good_cols       = [\"crimeaditionalinfo\"]\n",
    "if LOCAL_TEST:\n",
    "    train          = pd.read_csv('data/train.csv')\n",
    "    train = train[good_cols+['category', 'sub_category']]\n",
    "else:\n",
    "    train       = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "    test        = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')    \n",
    "    train, test = train[good_cols], test[good_cols]\n",
    "\n",
    "########################### Get basic helpers #################################################################################\n",
    "print('1.2. Basic helpers')\n",
    "bert_uncased_vocabulary = load_helper_file('helper_bert_uncased_vocabulary')\n",
    "bert_cased_vocabulary   = load_helper_file('helper_bert_cased_vocabulary')\n",
    "bert_char_list          = list(set([c for line in bert_uncased_vocabulary+bert_cased_vocabulary for c in line]))\n",
    "\n",
    "url_extensions          = load_helper_file('helper_url_extensions')\n",
    "html_tags               = load_helper_file('helper_html_tags')\n",
    "good_chars_dieter       = load_helper_file('helper_good_chars_dieter')\n",
    "bad_chars_dieter        = load_helper_file('helper_bad_chars_dieter')\n",
    "helper_contractions     = load_helper_file('helper_contractions')\n",
    "global_vocabulary       = load_helper_file('helper_global_vocabulary')\n",
    "global_vocabulary_chars = load_helper_file('helper_global_vocabulary_chars')\n",
    "normalized_chars        = load_helper_file('helper_normalized_chars')\n",
    "white_list_chars        = load_helper_file('helper_white_list_chars')\n",
    "white_list_punct        = \" '*-.,?!/:;_()[]{}<>=\" + '\"'\n",
    "pictograms_to_emoji     = load_helper_file('helper_pictograms_to_emoji')\n",
    "toxic_misspell_dict     = load_helper_file('helper_toxic_misspell_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Initial State:\n",
      "Unknown words: 132334 | Known words: 10607\n"
     ]
    }
   ],
   "source": [
    "data = train['crimeaditionalinfo']\n",
    "local_vocab = bert_uncased_vocabulary\n",
    "verbose = True\n",
    "global_lower=True\n",
    "data = data.astype(str)\n",
    "if verbose: print('#' *20 ,'Initial State:'); check_vocab(data, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Lowering everything:\n",
      "Unknown words: 100108 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "if global_lower:\n",
    "    data = data.apply(lambda x: x.lower())\n",
    "    if verbose: print('#'*10 ,'Step - Lowering everything:'); check_vocab(data, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize chars and dots:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Normalize chars and dots - SEE HELPER FOR DETAILS\n",
    "# Global\n",
    "data = data.apply(lambda x: ' '.join([make_cleaning(i,normalized_chars) for i in x.split()]))\n",
    "data = data.apply(lambda x: re.sub('\\(dot\\)', '.', x))\n",
    "data = data.apply(lambda x: deaccent(x))\n",
    "if verbose: print('#'*10 ,'Step - Normalize chars and dots:'); check_vocab(data, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Control Chars:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Remove 'control' chars\n",
    "# Global    \n",
    "global_chars_list = list(set([c for line in data for c in line]))\n",
    "chars_dict = {c:'' for c in global_chars_list if unicodedata.category(c)[0]=='C'}\n",
    "data = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#'*10 ,'Step - Control Chars:'); check_vocab(data, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove hrefs:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Remove hrefs\n",
    "# Global    \n",
    "data = data.apply(lambda x: re.sub(re.findall(r'\\<a(.*?)\\>', x)[0], '', x) if (len(re.findall(r'\\<a (.*?)\\>', x))>0) and ('href' in re.findall(r'\\<a (.*?)\\>', x)[0]) else x)\n",
    "if verbose: print('#'*10 ,'Step - Remove hrefs:'); check_vocab(data, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols:\n",
      "Unknown words: 100104 | Known words: 11551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert or remove Bad Symbols\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in data for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if (c not in bert_char_list) and (c not in emoji.EMOJI_DATA) and (c not in white_list_chars)])\n",
    "\n",
    "chars_dict = {}\n",
    "for char in chars:\n",
    "    try:\n",
    "        new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "        if len(new_char)==1:\n",
    "            chars_dict[ord(char)] = new_char\n",
    "        else:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    except:\n",
    "        chars_dict[ord(char)] = ''\n",
    "data = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove Bad Symbols:'); check_vocab(data, local_vocab)\n",
    "if verbose: print(chars)\n",
    "if verbose: print_dict(chars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols PART 2:\n",
      "Unknown words: 100104 | Known words: 11551\n",
      "·\n",
      "183 --- \n"
     ]
    }
   ],
   "source": [
    "# Remove Bad Symbols PART 2\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in data for c in line]))\n",
    "chars = '·' + ''.join([c for c in global_chars_list if (c not in white_list_chars) and (c not in emoji.EMOJI_DATA) and (c not in white_list_punct) and (ord(c)>256)])\n",
    "chars_dict = {}\n",
    "for char in chars:\n",
    "    try:\n",
    "        new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "        if len(new_char)==1:\n",
    "            chars_dict[ord(char)] = new_char\n",
    "        else:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    except:\n",
    "        chars_dict[ord(char)] = ''\n",
    "data = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove Bad Symbols PART 2:'); check_vocab(data, local_vocab)\n",
    "if verbose: print(chars)\n",
    "if verbose: print_dict(chars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - HTML tags:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Remove html tags\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in data for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if ('<' in word) and ('>' in word):\n",
    "        for tag in html_tags:\n",
    "            if ('<'+tag+'>' in word) or ('</'+tag+'>' in word):\n",
    "                temp_dict[word] = BeautifulSoup(word, 'html5lib').text  \n",
    "data = data.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - HTML tags:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 1:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Remove links (There is valuable information in links (probably you will find a way to use it)) \n",
    "# Global\n",
    "temp_vocab = list(set([c for line in data for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "url_rule = r'(?P<url>https?://[^\\s]+)'\n",
    "temp_dict = {k:domain_search(k) for k in temp_vocab if k!= re.compile(url_rule).sub('url', k)}\n",
    "    \n",
    "for word in temp_dict:\n",
    "    new_value = temp_dict[word]\n",
    "    if word.find('http')>2:\n",
    "        temp_dict[word] =  word[:word.find('http')] + ' ' + place_hold(new_value)\n",
    "    else:\n",
    "        temp_dict[word] = place_hold(new_value)\n",
    "            \n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 1:'); check_vocab(data, local_vocab); \n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 2:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Convert urls part 2\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in data for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "\n",
    "for word in temp_vocab:\n",
    "    url_check = False\n",
    "    if 'file:' in word:\n",
    "        url_check = True\n",
    "    elif ('http' in word) or ('ww.' in word) or ('.htm' in word) or ('ftp' in word) or ('.php' in word) or ('.aspx' in word):\n",
    "        if 'Aww' not in word:\n",
    "            for d_zone in url_extensions:\n",
    "                if '.' + d_zone in word:\n",
    "                    url_check = True\n",
    "                    break            \n",
    "    elif ('/' in word) and ('.' in word):\n",
    "        for d_zone in url_extensions:\n",
    "            if '.' + d_zone + '/' in word:\n",
    "                url_check = True\n",
    "                break\n",
    "\n",
    "    if url_check:\n",
    "        temp_dict[word] =  place_hold(domain_search(word))\n",
    "        \n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 2:'); check_vocab(data, local_vocab); \n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9]').sub('', word))>2:\n",
    "        for pict in pictograms_to_emoji:\n",
    "            if (pict in word) and (len(pict)>2):\n",
    "                temp_dict[word] = word.replace(pict, pictograms_to_emoji[pict])\n",
    "            elif pict==word:  \n",
    "                temp_dict[word] = pictograms_to_emoji[pict]\n",
    "\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Normalize pictograms:'); check_vocab(data, local_vocab); \n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate emoji:\n",
      "Unknown words: 100104 | Known words: 11551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Isolate emoji\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in data for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if c in emoji.EMOJI_DATA])\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "data = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Isolate emoji:'); check_vocab(data, local_vocab)\n",
    "if verbose: print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Duplicated Chars:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Duplicated dots, question marks and exclamations\n",
    "# Local\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if (Counter(word)['.']>1) or (Counter(word)['!']>1) or (Counter(word)['?']>1) or (Counter(word)[',']>1):\n",
    "        if (Counter(word)['.']>1):\n",
    "            new_word = re.sub('\\.\\.+', ' . . . ', new_word)\n",
    "        if (Counter(word)['!']>1):\n",
    "            new_word = re.sub('\\!\\!+', ' ! ! ! ', new_word)\n",
    "        if (Counter(word)['?']>1):\n",
    "            new_word = re.sub('\\?\\?+', ' ? ? ? ', new_word)\n",
    "        if (Counter(word)[',']>1):\n",
    "            new_word = re.sub('\\,\\,+', ' , , , ', new_word)\n",
    "        temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Duplicated Chars:'); check_vocab(data, local_vocab);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove underscore:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Remove underscore for spam words\n",
    "# Local\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and ('_' in word):\n",
    "        temp_dict[word] = re.sub('_', '', word)       \n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove underscore:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Spam chars repetition:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Isolate spam chars repetition\n",
    "# Local\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and (len(Counter(word))==1) and (len(word)>2):\n",
    "        temp_dict[word] = ' '.join([' ' + next(iter(Counter(word).keys())) + ' ' for i in range(3)])\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Spam chars repetition:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms part 2:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms part 2\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9]').sub('', word))>1:\n",
    "        for pict in pictograms_to_emoji:\n",
    "            if pict==word:  \n",
    "                temp_dict[word] = pictograms_to_emoji[pict]\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Normalize pictograms part 2:'); check_vocab(data, local_vocab); \n",
    "if verbose: print_dict(temp_dict)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Brackets and quotes:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Isolate brakets and quotes\n",
    "# Global\n",
    "chars = '()[]{}<>\"'\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "data = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Brackets and quotes:'); check_vocab(data, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Break short words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in data for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_vocab = [k for k in temp_vocab if len(k)<=20]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if '/' in word:\n",
    "        temp_dict[word] = re.sub('/', ' / ', word)\n",
    "    \n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(data, local_vocab); \n",
    "if verbose: print_dict(temp_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Break long words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in data for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_vocab = [k for k in temp_vocab if len(k)>20]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if '_' in word:\n",
    "        temp_dict[word] = re.sub('_', ' ', word)\n",
    "    elif '/' in word:\n",
    "        temp_dict[word] = re.sub('/', ' / ', word)\n",
    "    elif len(' '.join(word.split('-')).split())>2:\n",
    "        temp_dict[word] = re.sub('-', ' ', word)\n",
    "    \n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(data, local_vocab); \n",
    "if verbose: print_dict(temp_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Remove/Convert usernames and hashtags (add username/hashtag word?????)\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in data for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if (len(word) > 3) and (word[1:len(word)-1].isalnum()) and (not re.compile('[#@,.:;]').sub('', word).isnumeric()):\n",
    "        if word[len(word)-1].isalnum():\n",
    "            if (word.startswith('@')) or (word.startswith('#')):\n",
    "                new_word = place_hold(new_word[0] + ' ' + new_word[1:]) \n",
    "        else:\n",
    "            if (word.startswith('@')) or (word.startswith('#')):\n",
    "                new_word = place_hold(new_word[0] + ' ' + new_word[1:len(word)-1]) + ' ' + word[len(word)-1]\n",
    "\n",
    "    temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - UserName and Hashtag:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove ending underscore:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Remove ending underscore (or add quotation marks???)\n",
    "# Local\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if word[len(word)-1]=='_':\n",
    "        for i in range(len(word),0,-1):\n",
    "            if word[i-1]!='_':\n",
    "                new_word = word[:i]\n",
    "                temp_dict[word] = new_word   \n",
    "                break\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove ending underscore:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove starting underscore:\n",
      "Unknown words: 100104 | Known words: 11551\n"
     ]
    }
   ],
   "source": [
    "# Remove starting underscore \n",
    "# Local\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if word[0]=='_':\n",
    "        for i in range(len(word)):\n",
    "            if word[i]!='_':\n",
    "                new_word = word[i:]\n",
    "                temp_dict[word] = new_word   \n",
    "                break\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove starting underscore:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - End word punctuations:\n",
      "Unknown words: 100035 | Known words: 11553\n",
      "day. --- day .\n",
      "compromised. --- compromised .\n",
      "shattered, --- shattered ,\n",
      "then, --- then ,\n",
      "solution. --- solution .\n",
      "suddenly, --- suddenly ,\n",
      "honestly, --- honestly ,\n",
      "unathorised, --- unathorised ,\n",
      "doing, --- doing ,\n",
      "normal. --- normal .\n"
     ]
    }
   ],
   "source": [
    "# End word punctuations\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in data for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[len(k)-1].isalnum())]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    for i in range(len(word),0,-1):\n",
    "        if word[i-1].isalnum():\n",
    "            new_word = word[:i] + ' ' + word[i:]\n",
    "            break\n",
    "    temp_dict[word] = new_word     \n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - End word punctuations:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Start word punctuations:\n",
      "Unknown words: 100035 | Known words: 11553\n"
     ]
    }
   ],
   "source": [
    "# Start word punctuations\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in data for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[0].isalnum())]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    for i in range(len(word)):\n",
    "        if word[i].isalnum():\n",
    "            new_word = word[:i] + ' ' + word[i:]\n",
    "            break\n",
    "    temp_dict[word] = new_word     \n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Start word punctuations:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Find and replace acronims:\n",
      "Unknown words: 100035 | Known words: 11553\n"
     ]
    }
   ],
   "source": [
    "# Find and replace acronims\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (Counter(word)['.']>1) and (check_replace(word)):\n",
    "        if (domain_search(word)!='') and (('www' in word) or (Counter(word)['/']>3)):\n",
    "            temp_dict[word] = place_hold('url ' + domain_search(word))\n",
    "        else: \n",
    "            if (re.compile('[\\.\\,]').sub('', word) in local_vocab) and (len(re.compile('[0-9\\.\\,\\-\\/\\:]').sub('', word))>0):\n",
    "                temp_dict[word] =  place_hold(re.compile('[\\.\\,]').sub('', word))\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Find and replace acronims:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Contractions:\n",
      "Unknown words: 100035 | Known words: 11553\n",
      "it's --- word_placeholder[it___is]\n",
      "wasn't --- word_placeholder[was___not]\n",
      "didn't --- word_placeholder[did___not]\n",
      "hadn't --- word_placeholder[had___not]\n",
      "that's --- word_placeholder[that___is]\n",
      "there's --- word_placeholder[there___is]\n",
      "can't --- word_placeholder[cannot]\n",
      "don't --- word_placeholder[do___not]\n",
      "i've --- word_placeholder[i___have]\n",
      "isn't --- word_placeholder[is___not]\n"
     ]
    }
   ],
   "source": [
    "# Apply spellchecker for contractions\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (\"'\" in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if word in helper_contractions:\n",
    "        temp_dict[word] = place_hold(helper_contractions[word])\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Contractions:'); check_vocab(data, local_vocab)\n",
    "if verbose: print_dict(temp_dict)                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Possible obscene:\n",
      "Unknown words: 100035 | Known words: 11553\n"
     ]
    }
   ],
   "source": [
    "# Isolate obscene (or just keep the word????)\n",
    "# Local\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word)\n",
    "    if len(Counter(new_word))>2:\n",
    "        temp_dict[word] = place_hold('fuck')\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Possible obscene:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove \"s:\n",
      "Unknown words: 100033 | Known words: 11553\n",
      "everyone's --- everyone\n",
      "someone's --- someone\n"
     ]
    }
   ],
   "source": [
    "# Remove 's (DO WE NEED TO REMOVE IT???)\n",
    "# Local\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {k:k[:-2] for k in temp_vocab if (check_replace(k)) and (k.lower()[-2:]==\"'s\")}\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove \"s:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert backslash:\n",
      "Unknown words: 100033 | Known words: 11553\n"
     ]
    }
   ],
   "source": [
    "# Convert backslash\n",
    "# Global\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('\\\\' in k)]    \n",
    "temp_dict = {k:re.sub('\\\\\\\\+', ' / ', k) for k in temp_vocab}\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert backslash:'); check_vocab(data, local_vocab)\n",
    "if verbose: print_dict(temp_dict)                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Dup chars (with vocab check):\n",
      "Unknown words: 98422 | Known words: 11603\n",
      "ahh --- ah\n",
      "preevious --- previous\n",
      "thanksss --- thanks\n",
      "hoon --- hon\n",
      "livv --- liv\n",
      "produuct --- product\n",
      "buss --- bus\n",
      "someeone --- someone\n",
      "takk --- tak\n",
      "mangaa --- manga\n"
     ]
    }
   ],
   "source": [
    "# Try remove duplicated chars (not sure about this!!!!!)\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "temp_vocab_dup = []\n",
    "    \n",
    "for word in temp_vocab:\n",
    "    temp_vocab_dup.append(''.join(ch for ch, _ in itertools.groupby(word)))\n",
    "temp_vocab_dup = set(temp_vocab_dup)\n",
    "temp_vocab_dup = temp_vocab_dup.difference(temp_vocab_dup.difference(set(local_vocab)))\n",
    "            \n",
    "for word in temp_vocab:\n",
    "    new_word = ''.join(ch for ch, _ in itertools.groupby(word))\n",
    "    if new_word in temp_vocab_dup:\n",
    "        temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if (k != v) and (v in local_vocab)}\n",
    "    \n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Dup chars (with vocab check):'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate numbers:\n",
      "Unknown words: 98422 | Known words: 11603\n"
     ]
    }
   ],
   "source": [
    "# Isolate numbers\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if re.compile('[a-zA-Z]').sub('', word) == word:\n",
    "        if re.compile('[0-9]').sub('', word) != word:\n",
    "            temp_dict[word] = word\n",
    "\n",
    "global_chars_list = list(set([c for line in temp_dict for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if not c.isdigit()])\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}                \n",
    "temp_dict = {k:place_hold(make_cleaning(k,chars_dict)) for k in temp_dict}\n",
    "    \n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Isolate numbers:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Join dashes:\n",
      "Unknown words: 98422 | Known words: 11603\n"
     ]
    }
   ],
   "source": [
    "# Join dashes\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.sub('\\-\\-+', '-', word)\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    \n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Join dashes:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 98422 | Known words: 11603\n"
     ]
    }
   ],
   "source": [
    "# Try join word (Sloooow)\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (Counter(k)['-']>1)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = ''.join(['' if c in '-' else c for c in word])\n",
    "    if (new_word in local_vocab) and (len(new_word)>3):\n",
    "        temp_dict[word] = new_word    \n",
    "    \n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 98421 | Known words: 11604\n",
      "one-off --- one - off\n"
     ]
    }
   ],
   "source": [
    "# Try Split word\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9\\*]').sub('', word))>0:\n",
    "        chars = re.compile('[a-zA-Z0-9\\*]').sub('', word)\n",
    "        temp_dict[word] = ''.join([' ' + c + ' ' if c in chars else c for c in word])\n",
    "    \n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - L33T (with vocab check):\n",
      "Unknown words: 98421 | Known words: 11604\n"
     ]
    }
   ],
   "source": [
    "# L33T vocabulary (SLOW)\n",
    "# https://simple.wikipedia.org/wiki/Leet\n",
    "# Local (only unknown words)\n",
    "def convert_leet(word):\n",
    "    # basic conversion \n",
    "    word = re.sub('0', 'o', word)\n",
    "    word = re.sub('1', 'i', word)\n",
    "    word = re.sub('3', 'e', word)\n",
    "    word = re.sub('\\$', 's', word)\n",
    "    word = re.sub('\\@', 'a', word)\n",
    "    return word\n",
    "            \n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = convert_leet(word)\n",
    "    if (new_word!=word): \n",
    "        if (len(word)>2) and (new_word in local_vocab):\n",
    "            temp_dict[word] = new_word\n",
    "    \n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - L33T (with vocab check):'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Open Holded words:\n",
      "Unknown words: 98410 | Known words: 11604\n"
     ]
    }
   ],
   "source": [
    "# Open Holded words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in data for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (not check_replace(k))]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.sub('___', ' ', word[17:-1])\n",
    "data = data.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "data = data.apply(lambda x: ' '.join([i for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Open Holded words:'); check_vocab(data, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Multiple form:\n",
      "Unknown words: 97809 | Known words: 11657\n",
      "facebooks --- facebook\n",
      "onlines --- online\n",
      "madams --- madam\n",
      "dears --- dear\n",
      "compliants --- compliant\n",
      "furnitures --- furniture\n",
      "confirmations --- confirmation\n",
      "synonyms --- synonym\n",
      "harassments --- harassment\n",
      "spoils --- spoil\n"
     ]
    }
   ],
   "source": [
    "# Search multiple form\n",
    "# Local | example -> flashlights / flashlight -> False / True\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (k[-1:]=='s') and (len(k)>4)]\n",
    "temp_dict = {k:k[:-1] for k in temp_vocab if (k[:-1] in local_vocab)}\n",
    "data = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Multiple form:'); check_vocab(data, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove emoji from text:\n",
      "Removed emojis: []\n"
     ]
    }
   ],
   "source": [
    "# Convert emoji to text - modified to remove emojis\n",
    "temp_vocab = check_vocab(data, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (k in emoji.EMOJI_DATA)]\n",
    "data = data.apply(lambda x: ' '.join([i for i in x.split() if i not in temp_vocab]))\n",
    "if verbose: print('#' * 10, 'Step - Remove emoji from text:')\n",
    "if verbose: print('Removed emojis:', temp_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_df = pd.DataFrame({\n",
    "    \"category\": train[\"category\"],\n",
    "    \"sub_category\": train[\"sub_category\"],\n",
    "    \"crimeaditionalinfo\": data,\n",
    "})\n",
    "modified_df = modified_df.replace('', np.nan)\n",
    "modified_df = modified_df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]  # Remove stop words\n",
    "    return ' '.join(filtered_words)  # Join the filtered words back into a string\n",
    "modified_df['crimeaditionalinfo'] = modified_df['crimeaditionalinfo'].astype(str)\n",
    "\n",
    "modified_df['crimeaditionalinfo'] = modified_df['crimeaditionalinfo'].apply(remove_stop_words)\n",
    "modified_df.to_csv('test_cleaned_v2.csv', index=False)\n",
    "\n",
    "modified_df.to_csv('data/cleaned data/train.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
